"""Text compression with multi-reviewer iterative refinement."""

import json
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

from json_repair import repair_json

from ..llm import LLM
from ..topologization.api import Topologization
from .clue import Clue, extract_clues_from_topologization
from .markup import format_clue_as_book


def _extract_json_from_markdown(text: str) -> str:
    """Extract JSON from markdown code blocks.

    Args:
        text: Text that may contain markdown code blocks

    Returns:
        Extracted JSON string (or original text if no code block found)
    """
    # Try to extract JSON from markdown code block
    match = re.search(r"```(?:json)?\s*\n(.*?)\n```", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()


def _extract_compressed_text(full_response: str) -> str:
    """Extract the actual compressed text from AI's structured response.

    The AI may output in this format:
    ## Working Notes (internal)
    [thoughts...]
    ---
    ## Compressed Text (or åŽ‹ç¼©æ–‡æœ¬)
    [actual compressed text]
    ---
    [additional notes...]

    Args:
        full_response: Full response from AI

    Returns:
        Extracted compressed text only
    """
    # Try to find "## Compressed Text" or "## åŽ‹ç¼©æ–‡æœ¬" section
    # Support both English and Chinese headings
    match = re.search(
        r"##\s*(?:Compressed\s+Text|åŽ‹ç¼©æ–‡æœ¬)\s*\n+(.*?)(?:\n+---|\*\*CRITICAL\*\*|$)",
        full_response,
        re.DOTALL | re.IGNORECASE,
    )

    if match:
        compressed_text = match.group(1).strip()
        # Remove any ã€ã€‘ brackets if present at start/end
        compressed_text = re.sub(r"^ã€|ã€‘$", "", compressed_text).strip()
        return compressed_text

    # Fallback: if no section marker found, return the full response
    # (for backward compatibility with old format)
    return full_response.strip()


def _extract_thinking_text(full_response: str) -> str:
    """Extract thinking/working notes from AI response, excluding the compressed text section.

    Args:
        full_response: Full response from AI

    Returns:
        Thinking text only (everything before the compressed text section)
    """
    # Find the compressed text section
    match = re.search(
        r"##\s*(?:Compressed\s+Text|åŽ‹ç¼©æ–‡æœ¬)\s*",
        full_response,
        re.IGNORECASE,
    )

    if match:
        # Get everything before the compressed text section
        thinking = full_response[: match.start()].strip()
        return thinking

    # If no section found, return empty (backward compatibility)
    return ""


@dataclass
class ClueReviewerInfo:
    """Information about a clue for reviewing compressed text."""

    clue_id: int
    weight: float
    label: str  # Simple label like "first_label â†’ last_label"
    reviewer_info: str  # Natural language review guidelines


@dataclass
class ReviewResult:
    """Result from a single reviewer."""

    clue_id: int
    weight: float
    issues: list[dict]  # List of issue dicts with type, severity, missing_info/problem, suggestion


@dataclass
class CompressionVersion:
    """A single compression version with its score."""

    iteration: int
    text: str
    score: float  # Lower is better
    reviews: list[ReviewResult]


def compress_text(
    topologization: Topologization,
    intention: str,
    llm: LLM,
    compression_ratio: float = 0.2,
    max_iterations: int = 5,
    max_clues: int = 10,
    log_dir_path: Path | None = None,
) -> str:
    """Compress text from topologization using iterative refinement.

    Args:
        topologization: Topologization object with knowledge graph and snakes
        intention: User's reading intention
        llm: LLM instance
        compression_ratio: Target compression ratio (default: 0.2 = 20%)
        max_iterations: Number of iterations (default: 5)
        max_clues: Maximum number of clues (narrative threads) to generate (default: 10)
        log_dir_path: Directory for compression logs (default: None, no logging)

    Returns:
        Compressed text string
    """
    print("\n" + "=" * 60)
    print("=== Text Compression Pipeline ===")
    print("=" * 60)

    # Setup logging
    log_file = None
    if log_dir_path is not None:
        log_dir_path.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d %H-%M-%S")
        log_file = log_dir_path / f"compression {timestamp}.log"
        with open(log_file, "w", encoding="utf-8") as f:
            f.write("=== Text Compression Log ===\n")
            f.write(f"Started at: {timestamp}\n")
            f.write(f"Compression ratio target: {compression_ratio:.0%}\n")
            f.write(f"Max iterations: {max_iterations}\n")
            f.write("\n\n")

    # Step 1: Get original text
    print("\nStep 1: Loading original text...")
    original_text = _get_full_text(topologization)
    print(f"Original text length: {len(original_text)} characters")

    # Step 2: Extract clues from topologization (with merging)
    print("\nStep 2: Extracting clues from topologization...")
    clues = extract_clues_from_topologization(topologization, max_clues=max_clues)
    print(f"Extracted {len(clues)} clues (max: {max_clues}) from {len(list(topologization.snake_graph))} snakes")

    # Step 2.5: Generate marked text for compression (with high-retention chunks wrapped)
    print("\nStep 2.5: Generating marked text for compression...")
    marked_original_text = _get_marked_full_text(topologization, clues)
    print(f"Marked text length: {len(marked_original_text)} characters")

    # Step 3: Generate reviewer info for each clue
    print("\nStep 3: Generating clue reviewers...")
    clue_reviewers = _generate_clue_reviewers(clues, llm, topologization)
    print(f"Generated {len(clue_reviewers)} clue reviewers")

    # Step 4: Calculate target length
    target_length = int(len(original_text) * compression_ratio)
    print(f"Target length: {target_length} characters ({compression_ratio:.0%} compression)")

    # Step 5: Iterative compression - always run max_iterations times
    print(f"\nStep 4: Iterative compression ({max_iterations} iterations)...")
    versions: list[CompressionVersion] = []
    previous_compressed_text: str | None = None
    revision_feedback: str | None = None
    reviewer_histories: dict[int, tuple[str, str]] = {}  # clue_id -> (prev_compressed_text, prev_response)

    for iteration in range(1, max_iterations + 1):
        print(f"\n--- Iteration {iteration}/{max_iterations} ---")

        # Log iteration header and revision feedback BEFORE compression
        if log_file is not None:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(f"\n{'=' * 80}\n")
                f.write(f"ITERATION {iteration}/{max_iterations}\n")
                f.write(f"{'=' * 80}\n\n")

                # Show revision feedback if present (from previous iteration)
                if revision_feedback:
                    f.write("Revision Feedback (Compressor's View):\n")
                    f.write(f"{'-' * 80}\n")
                    f.write(revision_feedback)
                    f.write(f"\n{'-' * 80}\n\n")

        # 4.1 Compress text
        print("Compressing text...")
        full_response, compressed_text = _compress_iteration(
            marked_text=marked_original_text,
            target_length=target_length,
            compression_ratio=compression_ratio,
            clue_reviewers=clue_reviewers,
            previous_compressed_text=previous_compressed_text,
            revision_feedback=revision_feedback,
            llm=llm,
        )
        print(f"Compressed to {len(compressed_text)} characters")

        # Log compressed text result
        if log_file is not None:
            with open(log_file, "a", encoding="utf-8") as f:
                # Log thinking text (excluding compressed text section)
                thinking_text = _extract_thinking_text(full_response)
                if thinking_text and thinking_text.strip():
                    f.write("Thinking:\n")
                    f.write(f"{'-' * 80}\n")
                    f.write(thinking_text)
                    f.write(f"\n{'-' * 80}\n\n")

                # Log extracted compressed text with character count
                f.write(f"Compressed Text ({len(compressed_text)} characters):\n")
                f.write(f"{'-' * 80}\n")
                f.write(compressed_text)
                f.write(f"\n{'-' * 80}\n\n\n")

        # 4.2 Review with all reviewers
        print("Reviewing compressed text...")
        reviews, raw_responses = _review_compression(
            compressed_text=compressed_text,
            clue_reviewers=clue_reviewers,
            llm=llm,
            reviewer_histories=reviewer_histories if reviewer_histories else None,
        )

        # Log reviewer output summary to file
        if log_file is not None:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write("Reviewer Output Summary:\n")
                f.write(f"{'-' * 80}\n")
                for review in reviews:
                    issue_count = len(review.issues)
                    f.write(f"Clue {review.clue_id}: output {issue_count} issue(s)\n")
                f.write(f"{'-' * 80}\n\n")

        # 4.3 Calculate score (lower is better)
        score = _calculate_score(reviews)
        print(f"Issue score: {score:.2f} (lower is better)")

        # Store this version
        versions.append(
            CompressionVersion(
                iteration=iteration,
                text=compressed_text,
                score=score,
                reviews=reviews,
            )
        )

        # If score is 0 (perfect), stop early
        if score == 0:
            print("âœ“ Perfect compression achieved (score = 0)")
            break

        # 4.5 Collect feedback for next iteration
        if iteration < max_iterations:
            print("Preparing revision feedback...")

            # First, build the same sorted issue list as _collect_feedback() for logging
            if log_file is not None:
                severity_values = {"critical": 9, "major": 3, "minor": 1}
                all_issues_for_log = []
                for review in reviews:
                    for issue_index, issue in enumerate(review.issues):
                        severity = issue.get("severity", "minor").lower()
                        severity_value = severity_values.get(severity, 1)
                        all_issues_for_log.append(
                            {
                                "clue_id": review.clue_id,
                                "issue_index": issue_index,
                                "severity_value": severity_value,
                                "weight": review.weight,
                            }
                        )

                # Sort by same logic as _collect_feedback()
                all_issues_for_log.sort(key=lambda x: (-x["severity_value"], -x["weight"], x["issue_index"]))

                # Write Issue Index Table
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write("Issue Index Table (for debugging):\n")
                    f.write(f"{'-' * 80}\n")
                    for i, issue in enumerate(all_issues_for_log, 1):
                        f.write(f"  Issue #{i}: Clue {issue['clue_id']}, Index {issue['issue_index']}\n")
                    f.write(f"{'-' * 80}\n\n")
                    f.write(f"Score: {score:.2f}\n")
                    f.write(f"{'-' * 80}\n\n")

            revision_feedback = _collect_feedback(reviews, llm)
            previous_compressed_text = compressed_text

            # Update reviewer histories for next iteration
            for clue_id, raw_response in raw_responses.items():
                reviewer_histories[clue_id] = (compressed_text, raw_response)

    # Step 5: Select best version (lowest score)
    if not versions:
        raise RuntimeError("Compression failed: no versions generated")

    best_version = min(versions, key=lambda v: v.score)
    print(f"\nâœ“ Selected iteration {best_version.iteration} with score {best_version.score:.2f}")

    # Log final selection
    if log_file is not None:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"\n{'=' * 80}\n")
            f.write("FINAL SELECTION\n")
            f.write(f"{'=' * 80}\n\n")
            f.write(f"Selected: Iteration {best_version.iteration}/{max_iterations}\n")
            f.write(f"Score: {best_version.score:.2f}\n")
            f.write(f"Length: {len(best_version.text)} characters\n")
            f.write(f"Compression ratio: {len(best_version.text) / len(original_text):.1%}\n")
            f.write(f"\n{'=' * 80}\n\n")

            # Print remaining unresolved issues
            if best_version.score > 0:
                f.write("REMAINING UNRESOLVED ISSUES\n")
                f.write(f"{'=' * 80}\n\n")

                issue_number = 1
                for review in best_version.reviews:
                    for issue in review.issues:
                        severity = issue.get("severity", "minor").upper()
                        problem = issue.get("problem", "")
                        suggestion = issue.get("suggestion", "")

                        f.write(f"{issue_number}. [{severity}]\n")
                        f.write(f"   Problem: {problem}\n")
                        if suggestion:
                            f.write(f"   Suggestion: {suggestion}\n")
                        f.write("\n")
                        issue_number += 1

                f.write(f"{'=' * 80}\n\n")
            else:
                f.write("ðŸŽ‰ CONGRATULATIONS! ðŸŽ‰\n")
                f.write(f"{'=' * 80}\n\n")
                f.write("All issues have been successfully resolved!\n")
                f.write("The compressed text meets all quality requirements.\n")
                f.write(f"\n{'=' * 80}\n\n")

    print("\n" + "=" * 60)
    print("=== Compression Complete ===")
    print("=" * 60)
    print(f"Final length: {len(best_version.text)} characters")
    print(f"Compression ratio: {len(best_version.text) / len(original_text):.1%}")

    return best_version.text


def _get_full_text(topologization: Topologization) -> str:
    """Get full original text by reading all fragments in order.

    Args:
        topologization: Topologization object

    Returns:
        Full text string
    """
    fragments_dir = topologization.workspace_path / "fragments"
    fragment_files = sorted(fragments_dir.glob("fragment_*.json"), key=lambda p: int(p.stem.split("_")[1]))

    all_sentences = []
    for fragment_file in fragment_files:
        with open(fragment_file, encoding="utf-8") as f:
            fragment_data = json.load(f)

        # Handle both old format (list) and new format (dict with "sentences")
        if isinstance(fragment_data, list):
            sentences = fragment_data
        else:
            sentences = fragment_data["sentences"]

        for sentence in sentences:
            all_sentences.append(sentence["text"])

    return " ".join(all_sentences)


def _get_marked_full_text(topologization: Topologization, clues: list[Clue]) -> str:
    """Get full original text with high-retention chunks marked.

    Wraps verbatim/detailed retention chunks in <chunk retention="XXX"> tags
    to signal the compressor that these parts need special preservation.

    Args:
        topologization: Topologization object
        clues: List of Clue objects with chunks

    Returns:
        Full text string with high-retention chunks wrapped in <chunk> tags
    """
    # Collect all chunks from all clues
    all_chunks = []
    for clue in clues:
        all_chunks.extend(clue.chunks)

    # Use format_clue_as_book with wrap_high_retention=True
    # This will format the entire text with only high-retention chunks marked
    marked_text = format_clue_as_book(all_chunks, topologization, wrap_high_retention=True)

    return marked_text


def _generate_clue_reviewers(
    clues: list[Clue],
    llm: LLM,
    topologization: Topologization,
) -> list[ClueReviewerInfo]:
    """Generate reviewer info for each clue.

    Args:
        clues: List of Clue objects (already weight-normalized)
        llm: LLM instance
        topologization: Topologization object (for formatting chunks)

    Returns:
        List of ClueReviewerInfo objects
    """
    clue_reviewers = []

    # Find prompt template
    info_prompt_path = Path(__file__).parent.parent / "data" / "editor" / "clue_reviewer_generator.jinja"

    for clue in clues:
        print(f"  Processing clue {clue.clue_id}: {clue.label}")
        print(f"    Weight: {clue.weight:.2f}")
        if clue.is_merged:
            print(f"    Merged from snakes: {clue.source_snake_ids}")

        # Format chunks as book-like text with XML markup
        clue_text = format_clue_as_book(clue.chunks, topologization, full_markup=True)

        # Generate reviewer strategy
        info_system_prompt = llm.load_system_prompt(info_prompt_path)
        user_message = clue_text

        info_response = llm.request(
            system_prompt=info_system_prompt,
            user_message=user_message,
            temperature=0.3,
        )

        if not info_response:
            raise RuntimeError(f"Clue {clue.clue_id} reviewer info generation failed: LLM returned empty response")

        reviewer_info = info_response.strip()
        print(f"    Reviewer strategy generated ({len(reviewer_info)} chars)")

        clue_reviewers.append(
            ClueReviewerInfo(
                clue_id=clue.clue_id,
                weight=clue.weight,
                label=clue.label,
                reviewer_info=reviewer_info,
            )
        )

    print("\n  Clue weights (already normalized, sum = 1.0):")
    for cr in clue_reviewers:
        print(f"    Clue {cr.clue_id}: {cr.weight:.3f}")

    return clue_reviewers


def _compress_iteration(
    marked_text: str,
    target_length: int,
    compression_ratio: float,
    clue_reviewers: list[ClueReviewerInfo],
    previous_compressed_text: str | None,
    revision_feedback: str | None,
    llm: LLM,
) -> tuple[str, str]:
    """Perform one compression iteration.

    Args:
        marked_text: Marked original text with high-retention chunks wrapped in <chunk> tags
        target_length: Target length in characters
        compression_ratio: Compression ratio
        clue_reviewers: List of clue reviewer info
        previous_compressed_text: Previous iteration's compressed text (None for first iteration)
        revision_feedback: Feedback from previous iteration (None for first iteration)
        llm: LLM instance

    Returns:
        Tuple of (full_response, compressed_text):
        - full_response: Complete AI response including Working Notes
        - compressed_text: Extracted compressed text only
    """
    # Load prompt template (no longer includes revision_feedback)
    prompt_path = Path(__file__).parent.parent / "data" / "editor" / "text_compressor.jinja"

    # Calculate acceptable length range (Â±15% of target)
    acceptable_min = int(target_length * 0.85)
    acceptable_max = int(target_length * 1.15)

    system_prompt = llm.load_system_prompt(
        prompt_path,
        original_length=len(marked_text),
        target_length=target_length,
        compression_ratio=int(compression_ratio * 100),
        acceptable_min=acceptable_min,
        acceptable_max=acceptable_max,
    )

    # Build messages based on iteration
    # User message contains the marked original text
    user_message = marked_text

    if previous_compressed_text is None or revision_feedback is None:
        # First iteration: simple request
        response = llm.request(
            system_prompt=system_prompt,
            user_message=user_message,
            temperature=0.5,
        )
    else:
        # Subsequent iterations: include previous attempt and feedback
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": previous_compressed_text},
            {"role": "user", "content": revision_feedback},
        ]
        response = llm.request_with_history(
            messages=messages,
            temperature=0.5,
        )

    if not response:
        raise RuntimeError("Compression failed: LLM returned empty response")

    full_response = response.strip()
    # Use the full response as compressed text - don't try to extract sections
    compressed_text = full_response

    return full_response, compressed_text


def _review_compression(
    compressed_text: str,
    clue_reviewers: list[ClueReviewerInfo],
    llm: LLM,
    reviewer_histories: dict[int, tuple[str, str]] | None = None,
) -> tuple[list[ReviewResult], dict[int, str]]:
    """Review compressed text with all clue reviewers.

    Args:
        compressed_text: Compressed text to review
        clue_reviewers: List of clue reviewer info
        llm: LLM instance
        reviewer_histories: Optional dict mapping clue_id to (previous_compressed_text, previous_response)

    Returns:
        Tuple of (reviews, raw_responses):
        - reviews: List of ReviewResult objects
        - raw_responses: Dict mapping clue_id to raw response text
    """
    reviews = []
    raw_responses = {}
    prompt_path = Path(__file__).parent.parent / "data" / "editor" / "clue_reviewer.jinja"

    for cr in clue_reviewers:
        system_prompt = llm.load_system_prompt(
            prompt_path,
            thread_info=cr.reviewer_info,
        )

        # Check if this reviewer has history
        has_history = reviewer_histories is not None and cr.clue_id in reviewer_histories

        if has_history:
            prev_compressed_text, prev_response = reviewer_histories[cr.clue_id]
            # Build conversation history: system + user(prev) + assistant(prev) + user(current)
            # All user messages contain only compressed text, no intention
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prev_compressed_text},
                {"role": "assistant", "content": prev_response},
                {"role": "user", "content": compressed_text},
            ]
            response = llm.request_with_history(
                messages=messages,
                temperature=0.3,
            )
        else:
            # First iteration: simple request with only compressed text
            response = llm.request(
                system_prompt=system_prompt,
                user_message=compressed_text,
                temperature=0.3,
            )

        if not response:
            raise RuntimeError(f"Clue {cr.clue_id} review failed: LLM returned empty response")

        # Store raw response
        raw_responses[cr.clue_id] = response

        try:
            review_json = _extract_json_from_markdown(response)
            review_data = json.loads(repair_json(review_json))
        except json.JSONDecodeError as e:
            raise RuntimeError(f"Clue {cr.clue_id} review failed: JSON parse error - {e}") from e

        try:
            issues = review_data.get("issues", [])
            reviews.append(
                ReviewResult(
                    clue_id=cr.clue_id,
                    weight=cr.weight,
                    issues=issues,
                )
            )
            print(f"  Clue {cr.clue_id}: {len(issues)} issues")
        except (ValueError, KeyError) as e:
            raise RuntimeError(f"Clue {cr.clue_id} review failed: Invalid data format - {e}") from e

    return reviews, raw_responses


def _calculate_score(reviews: list[ReviewResult]) -> float:
    """Calculate issue score from reviews (lower is better).

    Score = sum of (severity * weight) for all issues
    - critical: 9
    - major: 3
    - minor: 1

    Args:
        reviews: List of ReviewResult objects

    Returns:
        Issue score (0 = perfect, higher = more problems)
    """
    severity_values = {"critical": 9, "major": 3, "minor": 1}

    total_score = 0.0
    for review in reviews:
        for issue in review.issues:
            severity = issue.get("severity", "minor").lower()
            severity_value = severity_values.get(severity, 1)
            total_score += severity_value * review.weight

    return total_score


def _collect_feedback(reviews: list[ReviewResult], llm: LLM) -> str:
    """Collect feedback from reviews for next iteration.

    Implements blind review: issues are aggregated and sorted by severity/importance,
    without revealing which snake raised them.

    Args:
        reviews: List of ReviewResult objects
        llm: LLM instance for loading template

    Returns:
        Formatted feedback string for compressor
    """
    severity_values = {"critical": 9, "major": 3, "minor": 1}

    # Collect all issues with metadata
    all_issues = []
    for review in reviews:
        for issue_index, issue in enumerate(review.issues):
            severity = issue.get("severity", "minor").lower()
            severity_value = severity_values.get(severity, 1)
            all_issues.append(
                {
                    "clue_id": review.clue_id,
                    "issue_index": issue_index,
                    "severity": severity,
                    "severity_value": severity_value,
                    "weight": review.weight,
                    "description": issue.get("missing_info") or issue.get("problem", "No description"),
                    "suggestion": issue.get("suggestion", ""),
                }
            )

    if not all_issues:
        return "No issues found - all reviewers are satisfied."

    # Sort by (severity_value DESC, weight DESC, issue_index ASC)
    all_issues.sort(key=lambda x: (-x["severity_value"], -x["weight"], x["issue_index"]))

    # Format issues description (max 9 issues shown) - for LLM feedback
    issues_lines = []
    visible_count = min(9, len(all_issues))
    hidden_count = len(all_issues) - visible_count

    for i, issue in enumerate(all_issues[:visible_count], 1):
        issues_lines.append(f"{i}. [{issue['severity'].upper()}]")
        issues_lines.append(f"   Problem: {issue['description']}")
        if issue["suggestion"]:
            issues_lines.append(f"   Suggestion: {issue['suggestion']}")
        issues_lines.append("")

    if hidden_count > 0:
        issues_lines.append(f"... and {hidden_count} more issues hidden (lower priority)")

    issues_description = "\n".join(issues_lines)

    # Load feedback template and render
    feedback_template_path = Path(__file__).parent.parent / "data" / "editor" / "revision_feedback.jinja"
    feedback_message = llm.load_system_prompt(
        feedback_template_path,
        issues_description=issues_description,
    )

    return feedback_message
